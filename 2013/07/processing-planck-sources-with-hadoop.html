<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Processing sources in Planck maps with Hadoop and Python | Andrea Zonca</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Processing sources in Planck maps with Hadoop and Python" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Purpose The purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software. Hadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet. It allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them (&quot;map&quot; step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the &quot;reduce&quot; algorithm, which typically aggregates them and then save them to the distributed file-system. Hadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features. Most scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial. Problem description The Planck collaboration (btw I&#39;m part of it...) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources: http://irsa.ipac.caltech.edu/Missions/planck.html Each catalog contains about 1000 sources, and the collaboration released the location and flux of each source. The purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source. In a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy. Sources All files are available on github: https://github.com/zonca/planck-sources-hadoop Hadoop setup I am running on the San Diego Supercomputing data intensive cluster Gordon: http://www.sdsc.edu/us/resources/gordon/ SDSC has a simplified Hadoop setup based on shell scripts, myHadoop , which allows running Hadoop as a regular PBS job. The most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon. Using Python with Hadoop-streaming Hadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language. One of the most common choices for scientific applications is Python. Application design Best way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region. Therefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature ( preprocessing.py ). Of course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that&#39;s something we do not worry about, that is exactly what Hadoop takes care of. Implementation Input data The pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes. Therefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row. Mapper mapper.py The mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source. In this simple scenario, the only returned key printed to stdout is &quot;SOURCENAME_10arcminmean&quot;. For example, we can run a serial test by running: echo plancktest/submaps/030_045_025 | ./mapper.py and the returned output is: PCCS1 030 G023.00+40.77_10arcminmean 4.49202e-04 PCCS1 030 G023.13+42.14_10arcminmean 3.37773e-04 PCCS1 030 G023.84+45.26_10arcminmean 4.69427e-04 PCCS1 030 G024.32+48.81_10arcminmean 3.79832e-04 PCCS1 030 G029.42+43.41_10arcminmean 4.11600e-04 Reducer There is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file. Hadoop call run.pbs The hadoop call is: $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoop*streaming*.jar -file $FOLDER/mapper.py -mapper $FOLDER/mapper.py -input /user/$USER/Input/* -output /user/$USER/Output So we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs. Hadoop run and results For testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes. Processing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes. The outputs of the mappers are then joined, sorted and written on a single file, see the output file output/SAMPLE_RESULT_part-00000 . See the full log sample_logs.txt extracted running: /opt/hadoop/bin/hadoop job -history output Comparison of the results with the catalog Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess. Conclusion The advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop. The main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming." />
<meta property="og:description" content="Purpose The purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software. Hadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet. It allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them (&quot;map&quot; step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the &quot;reduce&quot; algorithm, which typically aggregates them and then save them to the distributed file-system. Hadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features. Most scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial. Problem description The Planck collaboration (btw I&#39;m part of it...) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources: http://irsa.ipac.caltech.edu/Missions/planck.html Each catalog contains about 1000 sources, and the collaboration released the location and flux of each source. The purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source. In a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy. Sources All files are available on github: https://github.com/zonca/planck-sources-hadoop Hadoop setup I am running on the San Diego Supercomputing data intensive cluster Gordon: http://www.sdsc.edu/us/resources/gordon/ SDSC has a simplified Hadoop setup based on shell scripts, myHadoop , which allows running Hadoop as a regular PBS job. The most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon. Using Python with Hadoop-streaming Hadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language. One of the most common choices for scientific applications is Python. Application design Best way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region. Therefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature ( preprocessing.py ). Of course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that&#39;s something we do not worry about, that is exactly what Hadoop takes care of. Implementation Input data The pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes. Therefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row. Mapper mapper.py The mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source. In this simple scenario, the only returned key printed to stdout is &quot;SOURCENAME_10arcminmean&quot;. For example, we can run a serial test by running: echo plancktest/submaps/030_045_025 | ./mapper.py and the returned output is: PCCS1 030 G023.00+40.77_10arcminmean 4.49202e-04 PCCS1 030 G023.13+42.14_10arcminmean 3.37773e-04 PCCS1 030 G023.84+45.26_10arcminmean 4.69427e-04 PCCS1 030 G024.32+48.81_10arcminmean 3.79832e-04 PCCS1 030 G029.42+43.41_10arcminmean 4.11600e-04 Reducer There is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file. Hadoop call run.pbs The hadoop call is: $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoop*streaming*.jar -file $FOLDER/mapper.py -mapper $FOLDER/mapper.py -input /user/$USER/Input/* -output /user/$USER/Output So we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs. Hadoop run and results For testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes. Processing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes. The outputs of the mappers are then joined, sorted and written on a single file, see the output file output/SAMPLE_RESULT_part-00000 . See the full log sample_logs.txt extracted running: /opt/hadoop/bin/hadoop job -history output Comparison of the results with the catalog Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess. Conclusion The advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop. The main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming." />
<link rel="canonical" href="https://zonca.dev/2013/07/processing-planck-sources-with-hadoop.html" />
<meta property="og:url" content="https://zonca.dev/2013/07/processing-planck-sources-with-hadoop.html" />
<meta property="og:site_name" content="Andrea Zonca" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2013-07-15T08:16:00-05:00" />
<script type="application/ld+json">
{"headline":"Processing sources in Planck maps with Hadoop and Python","url":"https://zonca.dev/2013/07/processing-planck-sources-with-hadoop.html","dateModified":"2013-07-15T08:16:00-05:00","datePublished":"2013-07-15T08:16:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zonca.dev/2013/07/processing-planck-sources-with-hadoop.html"},"description":"Purpose The purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software. Hadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet. It allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them (&quot;map&quot; step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the &quot;reduce&quot; algorithm, which typically aggregates them and then save them to the distributed file-system. Hadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features. Most scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial. Problem description The Planck collaboration (btw I&#39;m part of it...) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources: http://irsa.ipac.caltech.edu/Missions/planck.html Each catalog contains about 1000 sources, and the collaboration released the location and flux of each source. The purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source. In a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy. Sources All files are available on github: https://github.com/zonca/planck-sources-hadoop Hadoop setup I am running on the San Diego Supercomputing data intensive cluster Gordon: http://www.sdsc.edu/us/resources/gordon/ SDSC has a simplified Hadoop setup based on shell scripts, myHadoop , which allows running Hadoop as a regular PBS job. The most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon. Using Python with Hadoop-streaming Hadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language. One of the most common choices for scientific applications is Python. Application design Best way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region. Therefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature ( preprocessing.py ). Of course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that&#39;s something we do not worry about, that is exactly what Hadoop takes care of. Implementation Input data The pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes. Therefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row. Mapper mapper.py The mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source. In this simple scenario, the only returned key printed to stdout is &quot;SOURCENAME_10arcminmean&quot;. For example, we can run a serial test by running: echo plancktest/submaps/030_045_025 | ./mapper.py and the returned output is: PCCS1 030 G023.00+40.77_10arcminmean 4.49202e-04 PCCS1 030 G023.13+42.14_10arcminmean 3.37773e-04 PCCS1 030 G023.84+45.26_10arcminmean 4.69427e-04 PCCS1 030 G024.32+48.81_10arcminmean 3.79832e-04 PCCS1 030 G029.42+43.41_10arcminmean 4.11600e-04 Reducer There is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file. Hadoop call run.pbs The hadoop call is: $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoop*streaming*.jar -file $FOLDER/mapper.py -mapper $FOLDER/mapper.py -input /user/$USER/Input/* -output /user/$USER/Output So we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs. Hadoop run and results For testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes. Processing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes. The outputs of the mappers are then joined, sorted and written on a single file, see the output file output/SAMPLE_RESULT_part-00000 . See the full log sample_logs.txt extracted running: /opt/hadoop/bin/hadoop job -history output Comparison of the results with the catalog Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess. Conclusion The advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop. The main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://zonca.dev/feed.xml" title="Andrea Zonca" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Andrea Zonca</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/consult/">Consulting</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Processing sources in Planck maps with Hadoop and Python</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2013-07-15T08:16:00-05:00" itemprop="datePublished">
        Jul 15, 2013
      </time>
    </p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#hpc">hpc</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#supercomputing">supercomputing</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Planck">Planck</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#hadoop">hadoop</a>
        
      
      </p>
    

      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/zonca/zonca-blog/tree/master/_posts/2013-07-15-processing-planck-sources-with-hadoop.md" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

    </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2>
 Purpose
</h2>
<div>
 The purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software.
</div>
<div>
 Hadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet.
</div>
<div>
 It allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them ("map" step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the "reduce" algorithm, which typically aggregates them and then save them to the distributed file-system.
</div>
<div>
 Hadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features.
</div>
<div>
 Most scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial.
</div>
<div>
 <a name="more">
 </a>
</div>
<h2>
 Problem description
</h2>
<div>
 The Planck collaboration (btw I'm part of it...) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources:
</div>
<div>
 <a href="http://irsa.ipac.caltech.edu/Missions/planck.html">
  http://irsa.ipac.caltech.edu/Missions/planck.html
 </a>
</div>
<div>
 Each catalog contains about 1000 sources, and the collaboration released the location and flux of each source.
</div>
<div>
 The purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source.
</div>
<div>
 In a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy.
</div>
<h2>
 Sources
</h2>
<p>All files are available on github:
<br /></p>
<div>
 <a href="https://github.com/zonca/planck-sources-hadoop">
  https://github.com/zonca/planck-sources-hadoop
 </a>
</div>
<h2>
 Hadoop setup
</h2>
<div>
 I am running on the San Diego Supercomputing data intensive cluster Gordon:
</div>
<div>
 <a href="http://www.sdsc.edu/us/resources/gordon/">
  http://www.sdsc.edu/us/resources/gordon/
 </a>
</div>
<div>
 SDSC has a simplified Hadoop setup based on shell scripts,
 <a href="http://www.sdsc.edu/us/resources/gordon/gordon_hadoop.html">
  myHadoop
 </a>
 , which allows running Hadoop as a regular PBS job.
</div>
<div>
 The most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon.
</div>
<h3>
 Using Python with Hadoop-streaming
</h3>
<div>
 Hadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language.
</div>
<div>
 One of the most common choices for scientific applications is Python.
</div>
<h3>
 Application design
</h3>
<div>
 Best way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region.
</div>
<div>
 Therefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature (
 <a href="https://github.com/zonca/planck-sources-hadoop/blob/master/preprocessing.py">
  preprocessing.py
 </a>
 ).
</div>
<div>
 Of course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that's something we do not worry about, that is exactly what Hadoop takes care of.
</div>
<h2>
 Implementation
</h2>
<h3>
 Input data
</h3>
<div>
 The pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes.
</div>
<div>
 Therefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row.
</div>
<h3>
 Mapper
</h3>
<div>
 <a href="https://github.com/zonca/planck-sources-hadoop/blob/master/mapper.py">
  mapper.py
 </a>
</div>
<div>
 <br />
</div>
<div>
 The mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source.
</div>
<div>
 In this simple scenario, the only returned key printed to stdout is "SOURCENAME_10arcminmean".
</div>
<div>
 For example, we can run a serial test by running:
</div>
<div>
 <br />
</div>
<div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   echo plancktest/submaps/030_045_025 | ./mapper.py
  </span>
 </div>
</div>
<div>
 <span style="font-family: Courier New, Courier, monospace;">
  <br />
 </span>
</div>
<div>
 <span style="font-family: inherit;">
  and the returned output is:
 </span>
</div>
<div>
 <span style="font-family: inherit;">
  <br />
 </span>
</div>
<div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   PCCS1 030 G023.00+40.77_10arcminmean
   <span class="Apple-tab-span" style="white-space: pre;">
   </span>
   4.49202e-04
  </span>
 </div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   PCCS1 030 G023.13+42.14_10arcminmean
   <span class="Apple-tab-span" style="white-space: pre;">
   </span>
   3.37773e-04
  </span>
 </div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   PCCS1 030 G023.84+45.26_10arcminmean
   <span class="Apple-tab-span" style="white-space: pre;">
   </span>
   4.69427e-04
  </span>
 </div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   PCCS1 030 G024.32+48.81_10arcminmean
   <span class="Apple-tab-span" style="white-space: pre;">
   </span>
   3.79832e-04
  </span>
 </div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   PCCS1 030 G029.42+43.41_10arcminmean
   <span class="Apple-tab-span" style="white-space: pre;">
   </span>
   4.11600e-04
  </span>
 </div>
 <div style="font-family: inherit;">
  <br />
 </div>
</div>
<h3>
 Reducer
</h3>
<div>
 There is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file.
</div>
<h3>
 Hadoop call
</h3>
<div>
 <a href="https://github.com/zonca/planck-sources-hadoop/blob/master/run.pbs">
  run.pbs
 </a>
</div>
<div>
 <br />
</div>
<div>
 The hadoop call is:
</div>
<div>
 <br />
</div>
<div>
 <div>
  <span style="font-family: Courier New, Courier, monospace;">
   <code>
    $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoop*streaming*.jar -file $FOLDER/mapper.py -mapper $FOLDER/mapper.py -input /user/$USER/Input/* -output /user/$USER/Output
   </code>
  </span>
 </div>
</div>
<div>
 <br />
</div>
<div>
 So we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs.
</div>
<h2>
 Hadoop run and results
</h2>
<div>
 For testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes.
</div>
<div>
 Processing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes.
</div>
<div>
 The outputs of the mappers are then joined, sorted and written on a single file, see the output file
</div>
<div>
 <a href="https://github.com/zonca/planck-sources-hadoop/blob/master/output/SAMPLE_RESULT_part-00000">
  output/SAMPLE_RESULT_part-00000
 </a>
 .
</div>
<div>
 See the full log
 <a href="https://github.com/zonca/planck-sources-hadoop/blob/master/sample_logs.txt">
  sample_logs.txt
 </a>
 extracted running:
</div>
<div>
 <span style="font-family: Courier New, Courier, monospace;">
  /opt/hadoop/bin/hadoop job -history output
 </span>
</div>
<h3>
 <span style="font-family: inherit;">
  Comparison of the results with the catalog
 </span>
</h3>
<div>
 <span style="font-family: inherit;">
  Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess.
 </span>
</div>
<div>
 <br />
 <div class="separator" style="clear: both; text-align: left;">
  <a href="/images/processing-planck-sources-with-hadoop_s1600_download.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;">
   <img border="0" src="/images/processing-planck-sources-with-hadoop_s1600_download.png" />
  </a>
 </div>
 <h2>
  Conclusion
 </h2>
 <div>
  The advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop.
 </div>
 <div>
  The main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming.
 </div>
 <div>
  <br />
 </div>
 <div class="separator" style="clear: both; text-align: center;">
  <br />
 </div>
 <div class="separator" style="clear: both; text-align: center;">
  <br />
 </div>
</div>

  </div><a class="u-url" href="/2013/07/processing-planck-sources-with-hadoop.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Tutorials and blog posts by Andrea Zonca: Python, Jupyter, Kubernetes</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/zonca" target="_blank" title="zonca"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/andreazonca" target="_blank" title="andreazonca"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
